{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem statement is to predict the potential business value of a person who has performed a specific activity. The business value outcome is defined by a yes/no field attached to each unique activity in the activity file. The outcome field indicates whether or not each person has completed the outcome within a fixed window of time after each unique activity was performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read people.csv...\n"
     ]
    }
   ],
   "source": [
    "print(\"Read people.csv...\")\n",
    "people = pd.read_csv(r'people.csv',\n",
    "                       dtype={'people_id': np.str,\n",
    "                              'activity_id': np.str,\n",
    "                              'char_38': np.int32},\n",
    "                       parse_dates=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load train.csv...\n"
     ]
    }
   ],
   "source": [
    "print(\"Load train.csv...\")\n",
    "train = pd.read_csv(r'act_train.csv',\n",
    "                        dtype={'people_id': np.str,\n",
    "                               'activity_id': np.str,\n",
    "                               'outcome': np.int8},\n",
    "                        parse_dates=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load test.csv...\n"
     ]
    }
   ],
   "source": [
    "print(\"Load test.csv...\")\n",
    "test = pd.read_csv(r'act_test.csv',\n",
    "                       dtype={'people_id': np.str,\n",
    "                              'activity_id': np.str},\n",
    "                       parse_dates=['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The activity file contains all of the unique activities (and the corresponding activity characteristics) that each person has performed over time. Each row in the activity file represents a unique activity performed by a person on a certain date. Each activity has a unique activity_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "---------------------\n",
      "TRAIN SET INFORMATION\n",
      "---------------------\n",
      "Shape of training set: (2197291, 15) \n",
      "\n",
      "Column Headers: ['people_id', 'activity_id', 'date', 'activity_category', 'char_1', 'char_2', 'char_3', 'char_4', 'char_5', 'char_6', 'char_7', 'char_8', 'char_9', 'char_10', 'outcome'] \n",
      "\n",
      "people_id                    object\n",
      "activity_id                  object\n",
      "date                 datetime64[ns]\n",
      "activity_category            object\n",
      "char_1                       object\n",
      "char_2                       object\n",
      "char_3                       object\n",
      "char_4                       object\n",
      "char_5                       object\n",
      "char_6                       object\n",
      "char_7                       object\n",
      "char_8                       object\n",
      "char_9                       object\n",
      "char_10                      object\n",
      "outcome                        int8\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print (\"\\n\\n---------------------\")\n",
    "print (\"TRAIN SET INFORMATION\")\n",
    "print (\"---------------------\")\n",
    "print (\"Shape of training set:\", train.shape, \"\\n\")\n",
    "print (\"Column Headers:\", list(train.columns.values), \"\\n\")\n",
    "print (train.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING SET INFORMATION\n",
      "========================\n",
      "\n",
      "'people_id' has 151295 unique values\n",
      "~~Listing up to 10 unique values~~\n",
      "['ppl_100' 'ppl_100002' 'ppl_100003' 'ppl_100006' 'ppl_100013' 'ppl_100019'\n",
      " 'ppl_100025' 'ppl_100028' 'ppl_100029' 'ppl_100032']\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'activity_id' has 2197291 unique values\n",
      "~~Listing up to 10 unique values~~\n",
      "['act2_1734928' 'act2_2434093' 'act2_3404049' 'act2_3651215' 'act2_4109017'\n",
      " 'act2_898576' 'act2_1233489' 'act2_1623405' 'act2_1111598' 'act2_1177453']\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'date' has 411 unique values\n",
      "~~Listing up to 10 unique values~~\n",
      "['2023-08-26T00:00:00.000000000' '2022-09-27T00:00:00.000000000'\n",
      " '2023-08-04T00:00:00.000000000' '2022-11-23T00:00:00.000000000'\n",
      " '2023-02-07T00:00:00.000000000' '2023-06-28T00:00:00.000000000'\n",
      " '2022-08-10T00:00:00.000000000' '2023-03-02T00:00:00.000000000'\n",
      " '2022-09-13T00:00:00.000000000' '2023-02-10T00:00:00.000000000']\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'activity_category' has 7 unique values\n",
      "['type 4' 'type 2' 'type 3' 'type 5' 'type 1' 'type 7' 'type 6']\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'char_1' has 52 unique values\n",
      "~~Listing up to 10 unique values~~\n",
      "[nan 'type 3' 'type 36' 'type 24' 'type 2' 'type 5' 'type 12' 'type 23'\n",
      " 'type 7' 'type 1']\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'char_2' has 33 unique values\n",
      "~~Listing up to 10 unique values~~\n",
      "[nan 'type 5' 'type 11' 'type 6' 'type 2' 'type 1' 'type 16' 'type 14'\n",
      " 'type 4' 'type 8']\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'char_3' has 12 unique values\n",
      "~~Listing up to 10 unique values~~\n",
      "[nan 'type 1' 'type 5' 'type 6' 'type 3' 'type 7' 'type 8' 'type 4'\n",
      " 'type 9' 'type 2']\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'char_4' has 8 unique values\n",
      "[nan 'type 1' 'type 3' 'type 2' 'type 4' 'type 6' 'type 5' 'type 7']\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'char_5' has 8 unique values\n",
      "[nan 'type 6' 'type 1' 'type 5' 'type 2' 'type 4' 'type 3' 'type 7']\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'char_6' has 6 unique values\n",
      "[nan 'type 3' 'type 1' 'type 2' 'type 4' 'type 5']\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'char_7' has 9 unique values\n",
      "[nan 'type 3' 'type 1' 'type 4' 'type 2' 'type 5' 'type 6' 'type 7'\n",
      " 'type 8']\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'char_8' has 19 unique values\n",
      "~~Listing up to 10 unique values~~\n",
      "[nan 'type 6' 'type 4' 'type 5' 'type 9' 'type 18' 'type 14' 'type 7'\n",
      " 'type 3' 'type 8']\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'char_9' has 20 unique values\n",
      "~~Listing up to 10 unique values~~\n",
      "[nan 'type 8' 'type 1' 'type 2' 'type 7' 'type 13' 'type 9' 'type 15'\n",
      " 'type 4' 'type 6']\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'char_10' has 6516 unique values\n",
      "~~Listing up to 10 unique values~~\n",
      "['type 76' 'type 1' 'type 1727' 'type 894' 'type 143' 'type 297' 'type 269'\n",
      " 'type 230' 'type 276' 'type 503']\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'outcome' has 2 unique values\n",
      "[0 1]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Features with missing values:\n",
      "['char_1 has 2039676 missing', 'char_2 has 2039676 missing', 'char_3 has 2039676 missing', 'char_4 has 2039676 missing', 'char_5 has 2039676 missing', 'char_6 has 2039676 missing', 'char_7 has 2039676 missing', 'char_8 has 2039676 missing', 'char_9 has 2039676 missing', 'char_10 has 157615 missing']\n",
      "\n",
      "\n",
      "Features with non-numeric values:\n",
      "['people_id', 'activity_id', 'date', 'activity_category', 'char_1', 'char_2', 'char_3', 'char_4', 'char_5', 'char_6', 'char_7', 'char_8', 'char_9', 'char_10']\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n"
     ]
    }
   ],
   "source": [
    "missing_values = []\n",
    "nonumeric_values = []\n",
    "\n",
    "print (\"TRAINING SET INFORMATION\")\n",
    "print (\"========================\\n\")\n",
    "\n",
    "for column in train:\n",
    "    # Find all the unique feature values\n",
    "    uniq = train[column].unique()\n",
    "    print (\"'{}' has {} unique values\" .format(column,uniq.size))\n",
    "    if (uniq.size > 10):\n",
    "        print(\"~~Listing up to 10 unique values~~\")\n",
    "    print (uniq[0:10])\n",
    "    print (\"\\n-----------------------------------------------------------------------\\n\")\n",
    "    \n",
    "    # Find features with missing values\n",
    "    if (True in pd.isnull(uniq)):\n",
    "        s = \"{} has {} missing\" .format(column, pd.isnull(train[column]).sum())\n",
    "        missing_values.append(s)\n",
    "    \n",
    "    # Find features with non-numeric values\n",
    "    for i in range (1, np.prod(uniq.shape)):\n",
    "        if (re.match('nan', str(uniq[i]))):\n",
    "            break\n",
    "        if not (re.search('(^\\d+\\.?\\d*$)|(^\\d*\\.?\\d+$)', str(uniq[i]))):\n",
    "            nonumeric_values.append(column)\n",
    "            break\n",
    "  \n",
    "print (\"\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\")\n",
    "print (\"Features with missing values:\\n{}\\n\\n\" .format(missing_values))\n",
    "print (\"Features with non-numeric values:\\n{}\" .format(nonumeric_values))\n",
    "print (\"\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The people file contains all of the unique people and their characteristics, who have performed activities over time. Each row in the people file represents a unique person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "---------------------\n",
      "PEOPLE SET INFORMATION\n",
      "---------------------\n",
      "Shape of training set: (189118, 41) \n",
      "\n",
      "Column Headers: ['people_id', 'char_1', 'group_1', 'char_2', 'date', 'char_3', 'char_4', 'char_5', 'char_6', 'char_7', 'char_8', 'char_9', 'char_10', 'char_11', 'char_12', 'char_13', 'char_14', 'char_15', 'char_16', 'char_17', 'char_18', 'char_19', 'char_20', 'char_21', 'char_22', 'char_23', 'char_24', 'char_25', 'char_26', 'char_27', 'char_28', 'char_29', 'char_30', 'char_31', 'char_32', 'char_33', 'char_34', 'char_35', 'char_36', 'char_37', 'char_38'] \n",
      "\n",
      "people_id            object\n",
      "char_1               object\n",
      "group_1              object\n",
      "char_2               object\n",
      "date         datetime64[ns]\n",
      "char_3               object\n",
      "char_4               object\n",
      "char_5               object\n",
      "char_6               object\n",
      "char_7               object\n",
      "char_8               object\n",
      "char_9               object\n",
      "char_10                bool\n",
      "char_11                bool\n",
      "char_12                bool\n",
      "char_13                bool\n",
      "char_14                bool\n",
      "char_15                bool\n",
      "char_16                bool\n",
      "char_17                bool\n",
      "char_18                bool\n",
      "char_19                bool\n",
      "char_20                bool\n",
      "char_21                bool\n",
      "char_22                bool\n",
      "char_23                bool\n",
      "char_24                bool\n",
      "char_25                bool\n",
      "char_26                bool\n",
      "char_27                bool\n",
      "char_28                bool\n",
      "char_29                bool\n",
      "char_30                bool\n",
      "char_31                bool\n",
      "char_32                bool\n",
      "char_33                bool\n",
      "char_34                bool\n",
      "char_35                bool\n",
      "char_36                bool\n",
      "char_37                bool\n",
      "char_38               int32\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print (\"\\n\\n---------------------\")\n",
    "print (\"PEOPLE SET INFORMATION\")\n",
    "print (\"---------------------\")\n",
    "print (\"Shape of training set:\", people.shape, \"\\n\")\n",
    "print (\"Column Headers:\", list(people.columns.values), \"\\n\")\n",
    "print (people.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEOPLE SET INFORMATION\n",
      "========================\n",
      "\n",
      "'people_id' has 189118 unique values\n",
      "~~Listing up to 10 unique values~~\n",
      "['ppl_100' 'ppl_100002' 'ppl_100003' 'ppl_100004' 'ppl_100006' 'ppl_10001'\n",
      " 'ppl_100010' 'ppl_100013' 'ppl_100019' 'ppl_100025']\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'char_1' has 2 unique values\n",
      "['type 2' 'type 1']\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'group_1' has 34224 unique values\n",
      "~~Listing up to 10 unique values~~\n",
      "['group 17304' 'group 8688' 'group 33592' 'group 22593' 'group 6534'\n",
      " 'group 25417' 'group 4204' 'group 45749' 'group 36096' 'group 18035']\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'char_2' has 3 unique values\n",
      "['type 2' 'type 3' 'type 1']\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'date' has 1196 unique values\n",
      "~~Listing up to 10 unique values~~\n",
      "['2021-06-29T00:00:00.000000000' '2021-01-06T00:00:00.000000000'\n",
      " '2022-06-10T00:00:00.000000000' '2022-07-20T00:00:00.000000000'\n",
      " '2022-07-27T00:00:00.000000000' '2022-10-14T00:00:00.000000000'\n",
      " '2022-09-01T00:00:00.000000000' '2023-01-24T00:00:00.000000000'\n",
      " '2023-03-26T00:00:00.000000000' '2022-08-26T00:00:00.000000000']\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'char_3' has 43 unique values\n",
      "~~Listing up to 10 unique values~~\n",
      "['type 5' 'type 28' 'type 4' 'type 40' 'type 6' 'type 8' 'type 14' 'type 7'\n",
      " 'type 10' 'type 1']\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'char_4' has 25 unique values\n",
      "~~Listing up to 10 unique values~~\n",
      "['type 5' 'type 9' 'type 8' 'type 25' 'type 6' 'type 7' 'type 10' 'type 1'\n",
      " 'type 12' 'type 2']\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'char_5' has 9 unique values\n",
      "['type 5' 'type 9' 'type 4' 'type 8' 'type 7' 'type 6' 'type 1' 'type 2'\n",
      " 'type 3']\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'char_6' has 7 unique values\n",
      "['type 3' 'type 2' 'type 4' 'type 1' 'type 6' 'type 5' 'type 7']\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'char_7' has 25 unique values\n",
      "~~Listing up to 10 unique values~~\n",
      "['type 11' 'type 5' 'type 16' 'type 8' 'type 1' 'type 7' 'type 9' 'type 20'\n",
      " 'type 2' 'type 17']\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'char_8' has 8 unique values\n",
      "['type 2' 'type 1' 'type 3' 'type 6' 'type 8' 'type 5' 'type 4' 'type 7']\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'char_9' has 9 unique values\n",
      "['type 2' 'type 4' 'type 1' 'type 3' 'type 6' 'type 8' 'type 5' 'type 9'\n",
      " 'type 7']\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'char_10' has 2 unique values\n",
      "[ True False]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'char_11' has 2 unique values\n",
      "[False  True]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'char_12' has 2 unique values\n",
      "[False  True]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'char_13' has 2 unique values\n",
      "[ True False]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'char_14' has 2 unique values\n",
      "[ True False]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'char_15' has 2 unique values\n",
      "[False  True]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'char_16' has 2 unique values\n",
      "[ True False]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'char_17' has 2 unique values\n",
      "[False  True]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'char_18' has 2 unique values\n",
      "[False  True]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'char_19' has 2 unique values\n",
      "[False  True]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'char_20' has 2 unique values\n",
      "[False  True]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'char_21' has 2 unique values\n",
      "[ True False]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'char_22' has 2 unique values\n",
      "[False  True]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'char_23' has 2 unique values\n",
      "[False  True]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'char_24' has 2 unique values\n",
      "[False  True]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'char_25' has 2 unique values\n",
      "[False  True]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'char_26' has 2 unique values\n",
      "[False  True]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'char_27' has 2 unique values\n",
      "[ True False]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'char_28' has 2 unique values\n",
      "[ True False]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'char_29' has 2 unique values\n",
      "[False  True]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'char_30' has 2 unique values\n",
      "[ True False]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'char_31' has 2 unique values\n",
      "[ True False]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'char_32' has 2 unique values\n",
      "[False  True]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'char_33' has 2 unique values\n",
      "[False  True]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'char_34' has 2 unique values\n",
      "[ True False]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'char_35' has 2 unique values\n",
      "[ True False]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'char_36' has 2 unique values\n",
      "[ True False]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'char_37' has 2 unique values\n",
      "[False  True]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "'char_38' has 101 unique values\n",
      "~~Listing up to 10 unique values~~\n",
      "[ 36  76  99  84  90   2  91   0  60 100]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Features with missing values:\n",
      "[]\n",
      "\n",
      "\n",
      "Features with non-numeric values:\n",
      "['people_id', 'char_1', 'group_1', 'char_2', 'date', 'char_3', 'char_4', 'char_5', 'char_6', 'char_7', 'char_8', 'char_9', 'char_10', 'char_11', 'char_12', 'char_13', 'char_14', 'char_15', 'char_16', 'char_17', 'char_18', 'char_19', 'char_20', 'char_21', 'char_22', 'char_23', 'char_24', 'char_25', 'char_26', 'char_27', 'char_28', 'char_29', 'char_30', 'char_31', 'char_32', 'char_33', 'char_34', 'char_35', 'char_36', 'char_37']\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n"
     ]
    }
   ],
   "source": [
    "missing_values = []\n",
    "nonumeric_values = []\n",
    "\n",
    "print (\"PEOPLE SET INFORMATION\")\n",
    "print (\"========================\\n\")\n",
    "\n",
    "for column in people:\n",
    "    # Find all the unique feature values\n",
    "    uniq = people[column].unique()\n",
    "    print (\"'{}' has {} unique values\" .format(column,uniq.size))\n",
    "    if (uniq.size > 10):\n",
    "        print(\"~~Listing up to 10 unique values~~\")\n",
    "    print (uniq[0:10])\n",
    "    print (\"\\n-----------------------------------------------------------------------\\n\")\n",
    "    \n",
    "    # Find features with missing values\n",
    "    if (True in pd.isnull(uniq)):\n",
    "        s = \"{} has {} missing\" .format(column, pd.isnull(people[column]).sum())\n",
    "        missing_values.append(s)\n",
    "    \n",
    "    # Find features with non-numeric values\n",
    "    for i in range (1, np.prod(uniq.shape)):\n",
    "        if (re.match('nan', str(uniq[i]))):\n",
    "            break\n",
    "        if not (re.search('(^\\d+\\.?\\d*$)|(^\\d*\\.?\\d+$)', str(uniq[i]))):\n",
    "            nonumeric_values.append(column)\n",
    "            break\n",
    "  \n",
    "print (\"\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\")\n",
    "print (\"Features with missing values:\\n{}\\n\\n\" .format(missing_values))\n",
    "print (\"Features with non-numeric values:\\n{}\" .format(nonumeric_values))\n",
    "print (\"\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    }
   ],
   "source": [
    "print (\"Processing...\")\n",
    "\n",
    "for table in [train, test]:\n",
    "        table['year'] = table['date'].dt.year\n",
    "        table['month'] = table['date'].dt.month\n",
    "        table['day'] = table['date'].dt.day\n",
    "        table.drop('date', axis=1, inplace=True)\n",
    "        table['activity_category'] = table['activity_category'].str.lstrip('type ').astype(np.int32)\n",
    "        for i in range(1, 11):\n",
    "            table['char_' + str(i)].fillna('type 0', inplace=True)\n",
    "            table['char_' + str(i)] = table['char_' + str(i)].str.lstrip('type ').astype(np.int32)\n",
    "\n",
    "people['year'] = people['date'].dt.year\n",
    "people['month'] = people['date'].dt.month\n",
    "people['day'] = people['date'].dt.day\n",
    "people.drop('date', axis=1, inplace=True)\n",
    "people['group_1'] = people['group_1'].str.lstrip('group ').astype(np.int32)\n",
    "for i in range(1, 10):\n",
    "    people['char_' + str(i)] = people['char_' + str(i)].str.lstrip('type ').astype(np.int32)\n",
    "for i in range(10, 38):\n",
    "    people['char_' + str(i)] = people['char_' + str(i)].astype(np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two separate data files must be joined together to create a single, unified data table: a people file and an activity file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge...\n"
     ]
    }
   ],
   "source": [
    "print(\"Merge...\")\n",
    "train = pd.merge(train, people, how='left', on='people_id', left_index=True)\n",
    "train.fillna(0.0, inplace=True)\n",
    "test = pd.merge(test, people, how='left', on='people_id', left_index=True)\n",
    "test.fillna(0.0, inplace=True)\n",
    "\n",
    "train = train.drop(['people_id'], axis=1)\n",
    "\n",
    "train.describe()\n",
    "\n",
    "#Separate label and data\n",
    "Y = train['outcome']\n",
    "X = train.drop(['outcome'], axis=1)\n",
    "\n",
    "X = X.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy of Logistic Regression: 0.8260721952622571\n"
     ]
    }
   ],
   "source": [
    "#Predict using Linear Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(max_iter=100)\n",
    "# Fit the model to our training data\n",
    "lr = lr.fit(X, Y)\n",
    "score = lr.score(X, Y)\n",
    "print(\"Mean accuracy of Logistic Regression: {0}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy of Random Forest: 0.9999995448941447\n"
     ]
    }
   ],
   "source": [
    "#Create the random forest object:\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(n_estimators=100)\n",
    "# Fit the model to our training data\n",
    "rfc = rfc.fit(X, Y)\n",
    "score = rfc.score(X, Y)\n",
    "print(\"Mean accuracy of Random Forest: {0}\".format(score))\n",
    "\n",
    "test = test.drop(['people_id'], axis=1)\n",
    "test_x = test.iloc[:, 1:]\n",
    "test_y = list(map(int, rfc.predict(test_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " #file for submission\n",
    "test['outcome'] = test_y\n",
    "test[['activity_id', 'outcome']] \\\n",
    "    .to_csv('results-rfc.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
